{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAVEL  DESTINATION RECOMMENDATION SYSTEM\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore future deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare the data\n",
    "\n",
    "Load the sample data into a suitable data structure, such as a pandas DataFrame.\n",
    "Preprocess the data if necessary, including handling missing values, converting categorical variables to numerical representations, and normalizing numerical features.\n",
    "Step 2: Split the data\n",
    "\n",
    "Split the data into training and testing sets. Typically, an 80-20 split is used, but you can adjust the ratio based on the size of your dataset.\n",
    "Step 3: Choose recommendation models\n",
    "\n",
    "There are several recommendation models you can choose from, depending on the nature of your data and the problem you want to solve. Here are a few popular models:\n",
    "Collaborative Filtering: This approach recommends items based on users' past behavior and preferences.\n",
    "Content-Based Filtering: This approach recommends items based on the similarity between items' characteristics and users' preferences.\n",
    "Matrix Factorization: This approach decomposes the user-item rating matrix to find latent factors and make recommendations.\n",
    "Neural Networks: You can also use deep learning models like neural networks for recommendation tasks.\n",
    "Step 4: Train and evaluate the models\n",
    "\n",
    "For each model you choose, train it using the training set.\n",
    "Evaluate the trained model's performance using appropriate evaluation metrics such as precision, recall, or Mean Average Precision (MAP).\n",
    "Repeat the training and evaluation process for each model.\n",
    "\n",
    "Step 5: Choose the best model\n",
    "\n",
    "Compare the performance of the different models based on the evaluation metrics.\n",
    "Select the model that performs best according to your evaluation criteria.\n",
    "\n",
    "Step 6: Fine-tune and optimize the chosen model\n",
    "\n",
    "Once you have selected the best model, you can further fine-tune and optimize its hyperparameters using techniques like cross-validation or grid search.\n",
    "\n",
    "Step 7: Deploy the recommendation system\n",
    "\n",
    "Once you are satisfied with the performance of your chosen and optimized model, you can deploy it to make real-time recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading 'clean_data' into df\n",
    "clean_df = pd.read_csv('Data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'type', 'subcategories', 'name', 'locationString', 'description',\n",
       "       'rating', 'latitude', 'longitude', 'numberOfReviews', 'amenities',\n",
       "       'LowerPrice', 'UpperPrice', 'RankingType', 'Rank', 'Total',\n",
       "       'regional_rating', 'country', 'city'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14484, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Dealing with outliers in the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numerical features for clustering\n",
    "numerical_columns = clean_df.select_dtypes(include=[np.number]).columns\n",
    "numerical_data = clean_df[numerical_columns]\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3)  # Specify the number of clusters\n",
    "kmeans.fit(numerical_data)\n",
    "\n",
    "# Assign each data point to a cluster\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Identify the cluster with the outliers\n",
    "outlier_cluster = np.argmax(np.bincount(labels))\n",
    "\n",
    "# Remove the rows belonging to the outlier cluster\n",
    "clean_df = clean_df[labels != outlier_cluster]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Cleaning and transforming textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategories</th>\n",
       "      <th>RankingType</th>\n",
       "      <th>locationString</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Specialty Lodging</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Rumangabo, North Kivu Province</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Rumangabo</td>\n",
       "      <td>Restaurant, Mountain View</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Internet, Room service, Free Internet, Free pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Goma, North Kivu Province</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Goma</td>\n",
       "      <td>Restaurant, Kids Activities, Suites, Room serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hotel</td>\n",
       "      <td>hotels</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Internet, Room service, Free Internet, Free pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hotel</td>\n",
       "      <td>hotels</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Kids Activities, Free parking, Restaurant, Bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14477</th>\n",
       "      <td>Specialty Lodging</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Rabil, Boa Vista</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Rabil</td>\n",
       "      <td>Internet, Suites, Free Internet, Free parking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Praia, Santiago</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Praia</td>\n",
       "      <td>Internet, Kids Activities, Room service, Free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>Specialty Lodging</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Sal Rei, Boa Vista</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Sal Rei</td>\n",
       "      <td>Internet, Free Internet, Free parking, Kitchen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Specialty lodging</td>\n",
       "      <td>Sao Filipe, Fogo</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Sao Filipe</td>\n",
       "      <td>Beachfront, Room service, Free parking, Restau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14483</th>\n",
       "      <td>Hotel</td>\n",
       "      <td>hotels</td>\n",
       "      <td>Tarrafal, Santiago</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Tarrafal</td>\n",
       "      <td>Beachfront, Room service, Restaurant, Bar/Loun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7596 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subcategories        RankingType                  locationString  \\\n",
       "0      Specialty Lodging  Specialty lodging  Rumangabo, North Kivu Province   \n",
       "3      Bed and Breakfast  Specialty lodging                        Kinshasa   \n",
       "12     Bed and Breakfast  Specialty lodging       Goma, North Kivu Province   \n",
       "14                 Hotel             hotels                        Kinshasa   \n",
       "16                 Hotel             hotels                        Kinshasa   \n",
       "...                  ...                ...                             ...   \n",
       "14477  Specialty Lodging  Specialty lodging                Rabil, Boa Vista   \n",
       "14478  Bed and Breakfast  Specialty lodging                 Praia, Santiago   \n",
       "14479  Specialty Lodging  Specialty lodging              Sal Rei, Boa Vista   \n",
       "14482  Bed and Breakfast  Specialty lodging                Sao Filipe, Fogo   \n",
       "14483              Hotel             hotels              Tarrafal, Santiago   \n",
       "\n",
       "                                country        city  \\\n",
       "0      Democratic Republic of the Congo   Rumangabo   \n",
       "3      Democratic Republic of the Congo    Kinshasa   \n",
       "12     Democratic Republic of the Congo        Goma   \n",
       "14     Democratic Republic of the Congo    Kinshasa   \n",
       "16     Democratic Republic of the Congo    Kinshasa   \n",
       "...                                 ...         ...   \n",
       "14477                        Cape Verde       Rabil   \n",
       "14478                        Cape Verde       Praia   \n",
       "14479                        Cape Verde     Sal Rei   \n",
       "14482                        Cape Verde  Sao Filipe   \n",
       "14483                        Cape Verde    Tarrafal   \n",
       "\n",
       "                                               amenities  \n",
       "0                              Restaurant, Mountain View  \n",
       "3      Internet, Room service, Free Internet, Free pa...  \n",
       "12     Restaurant, Kids Activities, Suites, Room serv...  \n",
       "14     Internet, Room service, Free Internet, Free pa...  \n",
       "16     Kids Activities, Free parking, Restaurant, Bar...  \n",
       "...                                                  ...  \n",
       "14477  Internet, Suites, Free Internet, Free parking,...  \n",
       "14478  Internet, Kids Activities, Room service, Free ...  \n",
       "14479  Internet, Free Internet, Free parking, Kitchen...  \n",
       "14482  Beachfront, Room service, Free parking, Restau...  \n",
       "14483  Beachfront, Room service, Restaurant, Bar/Loun...  \n",
       "\n",
       "[7596 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual_data = clean_df[['subcategories', 'RankingType', 'locationString', 'country', 'city', 'amenities']]\n",
    "textual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to categorical\n",
    "clean_df['type'] = clean_df['type'].astype('category')\n",
    "clean_df['amenities'] = clean_df['amenities'].astype('category')\n",
    "clean_df['subcategories'] = clean_df['subcategories'].astype('category')\n",
    "#clean_df['locationString'] = clean_df['locationString'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_subcategory_values = list(clean_df[\"subcategories\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "subcategory_map = {}\n",
    "for index, value in enumerate(unique_subcategory_values):\n",
    "    subcategory_map[value] = index + 1\n",
    "    \n",
    "# Create a new column with the encoded values\n",
    "clean_df['subcategories_mapped'] = clean_df['subcategories'].map(subcategory_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_ammenities_values = list(clean_df[\"amenities\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "amenities_mapping = {}\n",
    "for index, value in enumerate(unique_ammenities_values):\n",
    "    amenities_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"amenities_mapped\"] = clean_df[\"amenities\"].map(amenities_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_RankingType_values = list(clean_df[\"RankingType\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "RankingType_mapping = {}\n",
    "for index, value in enumerate(unique_RankingType_values):\n",
    "    RankingType_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"RankingType_mapped\"] = clean_df[\"RankingType\"].map(RankingType_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_RankingType_values = list(clean_df[\"RankingType\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "RankingType_mapping = {}\n",
    "for index, value in enumerate(unique_RankingType_values):\n",
    "    RankingType_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"RankingType_mapped\"] = clean_df[\"RankingType\"].map(RankingType_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_RankingType_values = list(clean_df[\"RankingType\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "RankingType_mapping = {}\n",
    "for index, value in enumerate(unique_RankingType_values):\n",
    "    RankingType_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"RankingType_mapped\"] = clean_df[\"RankingType\"].map(RankingType_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_country_values = list(clean_df[\"country\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "country_mapping = {}\n",
    "for index, value in enumerate(unique_country_values):\n",
    "    country_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"country_mapped\"] = clean_df[\"country\"].map(country_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique values in the column\n",
    "unique_type_values = list(clean_df[\"type\"].unique())\n",
    "\n",
    "# Create a dictionary that maps each unique value to a unique number\n",
    "type_mapping = {}\n",
    "for index, value in enumerate(unique_type_values):\n",
    "    type_mapping[value] = index + 1\n",
    "\n",
    "# Use the map() function to map the values in the column to their respective numbers\n",
    "clean_df[\"type_mapped\"] = clean_df[\"type\"].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>subcategories</th>\n",
       "      <th>name</th>\n",
       "      <th>locationString</th>\n",
       "      <th>description</th>\n",
       "      <th>rating</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfReviews</th>\n",
       "      <th>...</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Total</th>\n",
       "      <th>regional_rating</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>subcategories_mapped</th>\n",
       "      <th>amenities_mapped</th>\n",
       "      <th>RankingType_mapped</th>\n",
       "      <th>country_mapped</th>\n",
       "      <th>type_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8661504</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>Specialty Lodging</td>\n",
       "      <td>Bukima Tented Camp</td>\n",
       "      <td>Rumangabo, North Kivu Province</td>\n",
       "      <td>Just outside the Virunga National Park boundar...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>29.43</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Rumangabo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12274281</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Ixoras Hotel</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Located in Kinshasa, 10 km from Mbatu Museum, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.35</td>\n",
       "      <td>15.33</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6865307</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>Bed and Breakfast</td>\n",
       "      <td>Cap Kivu Hotel</td>\n",
       "      <td>Goma, North Kivu Province</td>\n",
       "      <td>Cap Kivu Hotel is an excellent choice for trav...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>29.21</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Goma</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10868306</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Hotel Selton</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Hotel selton is the new concept of hotel in th...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.36</td>\n",
       "      <td>15.21</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12237149</td>\n",
       "      <td>HOTEL</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Hotel Bella Riva</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>Finding an ideal budget friendly hotel in Kins...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>15.30</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.263158</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>Kinshasa</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   type      subcategories                name  \\\n",
       "0    8661504  HOTEL  Specialty Lodging  Bukima Tented Camp   \n",
       "3   12274281  HOTEL  Bed and Breakfast        Ixoras Hotel   \n",
       "12   6865307  HOTEL  Bed and Breakfast      Cap Kivu Hotel   \n",
       "14  10868306  HOTEL              Hotel        Hotel Selton   \n",
       "16  12237149  HOTEL              Hotel    Hotel Bella Riva   \n",
       "\n",
       "                    locationString  \\\n",
       "0   Rumangabo, North Kivu Province   \n",
       "3                         Kinshasa   \n",
       "12       Goma, North Kivu Province   \n",
       "14                        Kinshasa   \n",
       "16                        Kinshasa   \n",
       "\n",
       "                                          description  rating  latitude  \\\n",
       "0   Just outside the Virunga National Park boundar...     4.5     -1.38   \n",
       "3   Located in Kinshasa, 10 km from Mbatu Museum, ...     5.0     -4.35   \n",
       "12  Cap Kivu Hotel is an excellent choice for trav...     3.5     -1.68   \n",
       "14  Hotel selton is the new concept of hotel in th...     4.0     -4.36   \n",
       "16  Finding an ideal budget friendly hotel in Kins...     3.5     -4.30   \n",
       "\n",
       "    longitude  numberOfReviews  ...  Rank  Total  regional_rating  \\\n",
       "0       29.43               34  ...   2.0    3.0         1.500000   \n",
       "3       15.33                1  ...   9.0   67.0         7.444444   \n",
       "12      29.21               27  ...   3.0   17.0         5.666667   \n",
       "14      15.21               18  ...  10.0   43.0         4.300000   \n",
       "16      15.30                3  ...  19.0   43.0         2.263158   \n",
       "\n",
       "                             country       city  subcategories_mapped  \\\n",
       "0   Democratic Republic of the Congo  Rumangabo                     1   \n",
       "3   Democratic Republic of the Congo   Kinshasa                     2   \n",
       "12  Democratic Republic of the Congo       Goma                     2   \n",
       "14  Democratic Republic of the Congo   Kinshasa                     3   \n",
       "16  Democratic Republic of the Congo   Kinshasa                     3   \n",
       "\n",
       "    amenities_mapped RankingType_mapped country_mapped type_mapped  \n",
       "0                  1                  1              1           1  \n",
       "3                  2                  1              1           1  \n",
       "12                 3                  1              1           1  \n",
       "14                 4                  2              1           1  \n",
       "16                 5                  2              1           1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-eb61f5da2005>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Normalize the numerical columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnormalized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_columns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the numerical columns for normalization\n",
    "numerical_columns = ['rating', 'Rank', 'Total', 'regional_rating', 'LowerPrice', 'UpperPrice']\n",
    "\n",
    "# Normalize the numerical columns\n",
    "scaler = minmaxscaler()\n",
    "normalized_data = clean_df.copy()\n",
    "normalized_data[numerical_columns] = scaler(clean_df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 43.7478\n"
     ]
    }
   ],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'Rank']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "model = KNNBasic(random_state=42)\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.test(testset)\n",
    "accuracy = accuracy.rmse(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Error (RMSE) is a measure of the model's prediction accuracy. In the context of recommendation systems, it quantifies the average difference between the predicted ratings and the actual ratings given by the users. A lower RMSE value indicates better model performance. In this case, the RMSE is 41.8541, which suggests that the model's predictions have a relatively high level of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c0f301a4fbaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicted rating: {prediction.est:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Actual rating: {prediction.r_ui:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions1' is not defined"
     ]
    }
   ],
   "source": [
    "for prediction in predictions1:\n",
    "    print(f\"Predicted rating: {prediction.est:.2f}\")\n",
    "    print(f\"Actual rating: {prediction.r_ui:.2f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3  # Define the threshold for positive predictions\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for prediction in predictions1:\n",
    "    if prediction.est >= threshold:\n",
    "        if prediction.r_ui >= threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    elif prediction.r_ui >= threshold:\n",
    "        false_negatives += 1\n",
    "\n",
    "precision1 = true_positives / (true_positives + false_positives)\n",
    "recall1 = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f\"Precision: {precision1:.2f}\")\n",
    "print(f\"Recall: {recall1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It indicates how accurate the model is when it predicts positive instances. A precision score of 0.76 means that 76% of the instances predicted as positive were actually positive.\n",
    "\n",
    "Recall: Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positive instances. It indicates how well the model captures the positive instances. A recall score of 1.00 means that the model successfully identified all positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sup_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-90f6bb62c8f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpredictions2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0maccuracy2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msup_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sup_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'Rank']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model2 = SVD(random_state=42)\n",
    "model2.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions2 = model2.test(testset)\n",
    "accuracy2 = sup_accuracy.rmse(predictions2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of 44.0078 means that, on average, the predictions made by the model have an error of approximately 44.0078 units. The RMSE gives you an idea of how well your model's predictions align with the true values. The lower the RMSE, the better the model's performance.\n",
    "\n",
    "To further evaluate the significance of the RMSE value, it's important to consider the scale and context of your specific problem. Additionally, comparing the RMSE to the range of the target variable can provide insights into the relative performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions2:\n",
    "    print(f\"Predicted rating: {prediction.est:.2f}\")\n",
    "    print(f\"Actual rating: {prediction.r_ui:.2f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will iterate over the predictions and increment the corresponding counters based on the predicted ratings and actual ratings. Then, we calculate precision by dividing the number of true positives by the sum of true positives and false positives. Recall is calculated by dividing the number of true positives by the sum of true positives and false negatives.\n",
    "\n",
    "Note that this calculation assumes a binary classification problem where ratings above the threshold are considered positive and ratings below the threshold are considered negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4  # Define the threshold for positive predictions\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for prediction in predictions:\n",
    "    if prediction.est >= threshold:\n",
    "        if prediction.r_ui >= threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    elif prediction.r_ui >= threshold:\n",
    "        false_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It indicates how accurate the model is when it predicts positive instances. A precision score of 0.77 means that 77% of the instances predicted as positive were actually positive.\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positive instances. It indicates how well the model captures the positive instances. A recall score of 1.00 means that the model successfully identified all positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'regional_rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model3 = KNNBasic(random_state=42)\n",
    "model3.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions3 = model3.test(testset)\n",
    "accuracy3 = sup_accuracy.rmse(predictions3)\n",
    "\n",
    "threshold = 3  # Define the threshold for positive predictions\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for prediction in predictions3:\n",
    "    if prediction.est >= threshold:\n",
    "        if prediction.r_ui >= threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    elif prediction.r_ui >= threshold:\n",
    "        false_negatives += 1\n",
    "\n",
    "precision3 = true_positives / (true_positives + false_positives)\n",
    "recall3 = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f\"Precision: {precision3:.2f}\")\n",
    "print(f\"Recall: {recall3:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the RMSE value suggests that the model's predictions have an average deviation of 67.3796 from the actual ratings.\n",
    "A precision value of 0.70 means that out of all the recommendations predicted as positive by the model, 70% of them are actually relevant or accurate.\n",
    "A recall value of 1.00 means that out of all the actual positive recommendations, the model is able to identify and predict 100% of them accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'regional_rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model4 = SVD(random_state=42)\n",
    "model4.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions4 = model4.test(testset)\n",
    "accuracy4 = sup_accuracy.rmse(predictions4)\n",
    "\n",
    "threshold = 3  # Define the threshold for positive predictions\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for prediction in predictions4:\n",
    "    if prediction.est >= threshold:\n",
    "        if prediction.r_ui >= threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    elif prediction.r_ui >= threshold:\n",
    "        false_negatives += 1\n",
    "\n",
    "precision4 = true_positives / (true_positives + false_positives)\n",
    "recall4 = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f\"Precision: {precision4:.2f}\")\n",
    "print(f\"Recall: {recall4:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE value suggests that the model's predictions have an average deviation of ---- from the actual ratings. A precision value of ---- means that out of all the recommendations predicted as positive by the model, ----% of them are actually relevant or accurate. A recall value of ---- means that the model is able to identify and predict all of the actual positive recommendations accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'regional_rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model5 = NMF(random_state=42)\n",
    "model5.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions5 = model5.test(testset)\n",
    "accuracy5 = sup_accuracy.rmse(predictions5)\n",
    "\n",
    "threshold = 3  # Define the threshold for positive predictions\n",
    "\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for prediction in predictions5:\n",
    "    if prediction.est >= threshold:\n",
    "        if prediction.r_ui >= threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    elif prediction.r_ui >= threshold:\n",
    "        false_negatives += 1\n",
    "\n",
    "precision5 = true_positives / (true_positives + false_positives)\n",
    "recall5 = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f\"Precision: {precision5:.2f}\")\n",
    "print(f\"Recall: {recall5:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE value suggests that the model's predictions have an average deviation of ---- from the actual ratings. A precision value of ---- means that out of all the recommendations predicted as positive by the model, ----% of them are actually relevant or accurate. A recall value of ---- means that the model is able to identify and predict all of the actual positive recommendations accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> KNNWithMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with KNNwithMeans\n",
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'subcategories', 'rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the item-based collaborative filtering model\n",
    "model6 = KNNWithMeans(sim_options={'user_based': False})\n",
    "\n",
    "# Train the model\n",
    "model6.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions6 = model6.test(testset)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse_score6 = sup_accuracy.rmse(predictions6)\n",
    "#print(\"RMSE:\", rmse_score6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error (RMSE) for the predictions on the test set is 0.7981. RMSE is a measure of the difference between the predicted ratings and the actual ratings, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Surprise Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(clean_df[['id', 'rating', 'regional_rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model7 = KNNBasic(random_state=42)\n",
    "model7.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions7 = model7.test(testset)\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings = [pred.r_ui for pred in predictions7]\n",
    "predicted_ratings = [pred.est for pred in predictions7]\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r_squared = r2_score(actual_ratings, predicted_ratings)\n",
    "\n",
    "# Calculate the R-squared value using Surprise's accuracy module\n",
    "# r_squared = accuracy.rsquared(predictions)\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r_squared) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared value of -0.0697 suggests that the model's predictions do not explain much of the variance in the ratings. A negative R-squared value indicates that the model performs worse than a horizontal line (a model that predicts the average rating for all items). You may need to investigate further and consider other evaluation metrics to assess the performance of your recommendation model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the other models performance based on the r squared metric to see how they performed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings1 = [pred.r_ui for pred in predictions1]\n",
    "predicted_ratings1 = [pred.est for pred in predictions1]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings2 = [pred.r_ui for pred in predictions2]\n",
    "predicted_ratings2 = [pred.est for pred in predictions2]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings3 = [pred.r_ui for pred in predictions3]\n",
    "predicted_ratings3 = [pred.est for pred in predictions3]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings4 = [pred.r_ui for pred in predictions4]\n",
    "predicted_ratings4 = [pred.est for pred in predictions4]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings5 = [pred.r_ui for pred in predictions5]\n",
    "predicted_ratings5 = [pred.est for pred in predictions5]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings6 = [pred.r_ui for pred in predictions6]\n",
    "predicted_ratings6 = [pred.est for pred in predictions6]\n",
    "\n",
    "# Extract the actual ratings and predicted ratings from the predictions\n",
    "actual_ratings7 = [pred.r_ui for pred in predictions7]\n",
    "predicted_ratings7 = [pred.est for pred in predictions7]\n",
    "\n",
    "# List of predictions and corresponding names\n",
    "prediction_sets = [\n",
    "    (predictions1, \"Predictions 1\"),\n",
    "    (predictions2, \"Predictions 2\"),\n",
    "    (predictions3, \"Predictions 3\"),\n",
    "    (predictions4, \"Predictions 4\"),\n",
    "    (predictions5, \"Predictions 5\"),\n",
    "    (predictions6, \"Predictions 6\"),\n",
    "    (predictions7, \"Predictions 7\")\n",
    "]\n",
    "\n",
    "# Iterate over the prediction sets\n",
    "for predictions, name in prediction_sets:\n",
    "    # Extract the actual ratings and predicted ratings from the predictions\n",
    "    actual_ratings = [pred.r_ui for pred in predictions]\n",
    "    predicted_ratings = [pred.est for pred in predictions]\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Results for\", name)\n",
    "    print(\"Actual Ratings:\", actual_ratings)\n",
    "    print(\"Predicted Ratings:\", predicted_ratings)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of actual ratings and predicted ratings\n",
    "actual_ratings_list = [actual_ratings1, actual_ratings2, actual_ratings3, actual_ratings4, actual_ratings5, actual_ratings6, actual_ratings7 ]\n",
    "predicted_ratings_list = [predicted_ratings1, predicted_ratings2, predicted_ratings3, predicted_ratings4, predicted_ratings5, predicted_ratings6, predicted_ratings7]\n",
    "\n",
    "# Loop through the ratings lists\n",
    "for i in range(len(actual_ratings_list)):\n",
    "    actual_ratings = actual_ratings_list[i]\n",
    "    predicted_ratings = predicted_ratings_list[i]\n",
    "    \n",
    "    # Calculate the R-squared value\n",
    "    r_squared = r2_score(actual_ratings, predicted_ratings)\n",
    "    \n",
    "    # Print the R-squared value\n",
    "    print(f\"R-squared for Set {i+1}: {r_squared}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher value indicates a better fit of the model to the data. In this case, the R-squared values are negative, which suggests that the model does not fit the data well and may not be providing meaningful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unclear Modelling section\n",
    "Seek clarification (IAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a relevant columns from the above dataset \n",
    "vectorization_columns = clean_df[['name', 'subcategories', 'amenities']]\n",
    "vectorization_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant data into a list of strings\n",
    "documents = []\n",
    "for _, row in vectorization_columns.iterrows():\n",
    "    name = row['name']\n",
    "    subcategories = row['subcategories']\n",
    "    amenities = row['amenities']\n",
    "    doc = f\"{name} {subcategories} {amenities}\"\n",
    "    documents.append(doc)\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_recommendations(item_index, cosine_similarities, top_n=5):\n",
    "    # Get similarity scores for the item\n",
    "    item_scores = list(enumerate(cosine_similarities[item_index]))\n",
    "\n",
    "    # Sort items based on similarity scores\n",
    "    item_scores = sorted(item_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top-N similar items\n",
    "    top_items = item_scores[1 : top_n + 1]  # Exclude the item itself\n",
    "\n",
    "    return top_items\n",
    "\n",
    "# Get recommendations for a specific item (e.g., item with index 0)\n",
    "item_index = 0\n",
    "recommendations = get_item_recommendations(item_index, cosine_similarities)\n",
    "\n",
    "# Print the top 5 recommendations\n",
    "for item_id, similarity in recommendations:\n",
    "    print(f\"Item ID: {item_id}, Similarity: {similarity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8 (Part of Unclear section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the TF-IDF Matrix\n",
    "tfidfv2=TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "tfidfv_matrix2=tfidfv2.fit_transform(clean_df['amenities'])\n",
    "print(tfidfv_matrix2.todense())\n",
    "tfidfv_matrix2.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "cosine_sim2 = cosine_similarity(tfidfv_matrix2, tfidfv_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Series to map movie titles to their indices\n",
    "indices = pd.Series(data = list(clean_df.index), index = clean_df['name'])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_place(name, cosine_sim2, data):\n",
    "    # Create a dictionary to map movie titles to their indices\n",
    "    indices = {title: index for index, title in enumerate(clean_df['name'])}\n",
    "\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim2[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    indices = [x for x, _ in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    recommended_place = clean_df.iloc[indices]['name']\n",
    "    return recommended_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_place(\"St. Catherine's Monastery Guesthouse\", cosine_sim2, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_amenities(amenity, cosine_sim2, data):\n",
    "    # Create a dictionary to map movie titles to their indices\n",
    "    indices = {title: index for index, title in enumerate(clean_df['amenities'])}\n",
    "\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[amenities]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim2[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    indices = [x for x, _ in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    recommended_amenities = clean_df.iloc[indices]['amenities']\n",
    "    return recommended_amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_amenities(amenity, cosine_sim2, data):\n",
    "    # Create a dictionary to map amenity titles to their indices\n",
    "    indices = {title: index for index, title in enumerate(clean_df['amenities'])}\n",
    "\n",
    "    # Get the index of the amenity that matches the title\n",
    "    idx = indices[amenity]\n",
    "\n",
    "    # Get the pairwise similarity scores of all amenities with that amenity\n",
    "    sim_scores = list(enumerate(cosine_sim2[idx]))\n",
    "\n",
    "    # Sort the amenities based on the similarity scores\n",
    "    sim_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar amenities\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the amenity indices\n",
    "    indices = [x for x, _ in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar amenities\n",
    "    recommended_amenities = clean_df.iloc[indices]['amenities']\n",
    "    return recommended_amenities\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_recommendation_system(test_set, cosine_sim, data):\n",
    "    # Initialize evaluation metrics\n",
    "    accuracy = 0\n",
    "    rmse = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    total_test_cases = len(test_set)\n",
    "\n",
    "    # Iterate over each test case\n",
    "    for test_case in test_set:\n",
    "        ground_truth_amenity = test_case['ground_truth_amenity']\n",
    "        predicted_amenities = recommend_amenities(ground_truth_amenity, cosine_sim, data)\n",
    "        \n",
    "        # Evaluate metrics for the current test case\n",
    "        # Compare predicted_amenities with ground truth amenities\n",
    "        # Calculate accuracy, RMSE, precision, and recall\n",
    "\n",
    "    # Calculate average metrics\n",
    "    accuracy /= total_test_cases\n",
    "    rmse = np.sqrt(rmse / total_test_cases)\n",
    "    precision /= total_test_cases\n",
    "    recall /= total_test_cases\n",
    "\n",
    "    # Return the evaluation metrics\n",
    "    return accuracy, rmse, precision, recall\n",
    "\n",
    "# Test set with ground truth amenities\n",
    "test_set = [\n",
    "    {'ground_truth_amenity': 'Restaurant'},\n",
    "    {'ground_truth_amenity': 'Pool'},\n",
    "    # Add more test cases here...\n",
    "]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy, rmse, precision, recall = evaluate_recommendation_system(test_set, cosine_sim2, data)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of unclear section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem:\n",
    "    def __init__(self, clean_df, tfidfv_matrix2, cosine_sim2, cosine_similarities, indices):\n",
    "        self.clean_df = clean_df\n",
    "        self.tfidfv_matrix2 = tfidfv_matrix2\n",
    "        self.cosine_sim2 = cosine_sim2\n",
    "        self.cosine_similarities = cosine_similarities\n",
    "        self.indices = indices\n",
    "\n",
    "    def recommend_attraction(self, rating_threshold):\n",
    "        # Filter the DataFrame based on the rating threshold\n",
    "        recommendations = self.clean_df[self.clean_df['rating'] > rating_threshold][['name', 'LowerPrice', 'UpperPrice','amenities', 'type', 'country']]\n",
    "\n",
    "        # Reset the index of the recommendations DataFrame\n",
    "        recommendations.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def recommend_amenities(self, query):\n",
    "        # Check if the specified amenity exists in the dataset\n",
    "        if query not in self.clean_df['amenities'].str.join(', '):\n",
    "            st.error(f\"Error: '{query}' does not exist in the dataset.\")\n",
    "            return None\n",
    "\n",
    "        # Convert the string representation of amenities back into a list\n",
    "        self.clean_df['amenities'] = self.clean_df['amenities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "        # Get the index of the specified amenity\n",
    "        indices = self.clean_df['amenities'].apply(lambda x: query in x if isinstance(x, list) else False)\n",
    "\n",
    "        # Get the pairwise similarity scores of all items with the specified amenity\n",
    "        sim_scores = self.cosine_sim2[indices]\n",
    "\n",
    "        # Flatten the similarity scores\n",
    "        sim_scores = sim_scores.flatten()\n",
    "\n",
    "        # Get the indices of the sorted similarity scores\n",
    "        indices = np.argsort(sim_scores)[::-1]\n",
    "\n",
    "        # Get the sorted similarity scores\n",
    "        sim_scores = sim_scores[indices]\n",
    "\n",
    "        # Get the recommended items\n",
    "        recommended_items = self.clean_df.iloc[indices]\n",
    "\n",
    "        return recommended_items\n",
    "\n",
    "    def recommend_place(self, name):\n",
    "        # Create a dictionary to map place names to their indices\n",
    "        indices = {title: index for index, title in enumerate(self.clean_df['name'])}\n",
    "\n",
    "        # Check if the specified place exists in the dataset\n",
    "        if name not in indices:\n",
    "            st.error(f\"Error: '{name}' does not exist in the dataset.\")\n",
    "            return None\n",
    "\n",
    "        # Get the index of the specified place\n",
    "        idx = indices[name]\n",
    "\n",
    "        # Get the pairwise similarity scores of all places with the specified place\n",
    "        sim_scores = list(enumerate(self.cosine_similarities[idx]))\n",
    "\n",
    "        # Sort the places based on the similarity scores\n",
    "        sim_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the scores of the 10 most similar places\n",
    "        sim_scores = sim_scores[1:11]\n",
    "\n",
    "        # Get the indices of the top-N similar places\n",
    "        indices = [x for x, _ in sim_scores]\n",
    "\n",
    "        # Get the recommended places\n",
    "        recommended_places = self.clean_df.iloc[indices]['name']\n",
    "\n",
    "        return recommended_places\n",
    "\n",
    "    def get_item_recommendations(self, item_index, top_n=5):\n",
    "        # Get similarity scores for the item\n",
    "        item_scores = list(enumerate(self.cosine_similarities[item_index]))\n",
    "\n",
    "        # Sort items based on similarity scores\n",
    "        item_scores = sorted(item_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get top-N similar items\n",
    "        top_items = item_scores[1:top_n + 1]  # Exclude the item itself\n",
    "\n",
    "        return top_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs clarification (IAN)\n",
    "hybrid = RecommenderSystem('clean_df', 'tfidfv_matrix2', 'cosine_sim2', 'cosine_similarities', 'indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs Clarification (IAN)\n",
    "recommend_place('Excalibur Boutique Hotel', cosine_sim2, clean_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
